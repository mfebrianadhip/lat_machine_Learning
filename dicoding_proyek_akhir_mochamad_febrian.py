# -*- coding: utf-8 -*-
"""dicoding_proyek_akhir_mochamad_febrian.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HpW4phqyWPjrBHMFJEtxW0_bR-oSUlRo

# Proyek Akhir : Image Classification Model Deployment
**Kriteria Submission**
- Dataset yang akan dipakai bebas, namun minimal memiliki 1000 buah gambar.
- Dataset dibagi menjadi 80% train set dan 20% test set.
- Model harus menggunakan model sequential.
- Model harus menggunakan Conv2D Maxpooling Layer.
- Akurasi pada training dan validation set minimal sebesar 80%.
- Menggunakan Callback.
- Membuat plot terhadap akurasi dan loss model.
- Menulis kode untuk menyimpan model ke dalam format TF-Lite.

# Mengunduh data set

- Mengunduh data set menggunakan API dari kaggle.com
"""

!pip install kaggle

import os
os.environ['KAGGLE_CONFIG_DIR']='/content/'

!kaggle datasets download -d puneet6060/intel-image-classification

"""- Mengekstrak file zip"""

import zipfile

local_zip = '/content/intel-image-classification.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

"""# Membuat Dataset Training dan Validasi"""

class_names = ['buildings','forest','glacier','mountain','sea','street']
class_names_label = {class_name:i for i, class_name in enumerate(class_names)}

nb_classes = len(class_names)

IMAGE_SIZE = (150, 150)

import numpy as np
import cv2
import os
import tensorflow as tf                
from tqdm import tqdm
from sklearn.utils import shuffle 

def data_image():
    datasets = ['/content/seg_train/seg_train', '/content/seg_test/seg_test']
    output = []

    for dataset in datasets:
        images = []
        labels = []
        print("Loading {}".format(dataset))
        for folder in os.listdir(dataset):
            label = class_names_label[folder]
            for file in tqdm(os.listdir(os.path.join(dataset, folder))):
                img_path = os.path.join(os.path.join(dataset, folder), file)
                image = cv2.imread(img_path)
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image = cv2.resize(image, IMAGE_SIZE) 
                images.append(image)
                labels.append(label)
        images = np.array(images, dtype = 'float32')
        labels = np.array(labels, dtype = 'int32')   
        output.append((images, labels))
    return output

(train_images, train_labels), (test_images, test_labels) = data_image()

train_images, train_labels = shuffle(train_images, train_labels, random_state=32)

n_train = train_labels.shape[0]
n_test = test_labels.shape[0]

print ("Jumlah data training: {}".format(n_train))
print ("Jumlahh data test: {}".format(n_test))
print ("Ukuran setiap gambar : {}".format(IMAGE_SIZE))

train_images = train_images / 255.0 

test_images = test_images / 255.0

"""- Display gambar secara random"""

import matplotlib.pyplot as plt
import matplotlib.image

def display_images(class_names, images, labels):
    fig = plt.figure(figsize=(10,10))
    fig.suptitle('Beberapa gambar dalam dataset', fontsize=20)
    for i in range(25):
        plt.subplot(5,5,i+1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(images[i], cmap=plt.cm.binary)
        plt.xlabel(class_names[labels[i]])
    plt.show()

display_images(class_names, train_images, train_labels)

"""- Membuat image data generator dan membagi data training dan validation"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255.,
                                   horizontal_flip=True,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   validation_split=0.1)

train_generator=train_datagen.flow_from_directory(
      '/content/seg_train/seg_train',
      target_size=(150,150),
      batch_size=64,
      class_mode='sparse',
      seed=2209,
      subset='training')

validation_generator=train_datagen.flow_from_directory(
      '/content/seg_train/seg_train',
      target_size=(150,150),
      batch_size=64,
      class_mode='sparse',
      seed=2209,
      subset='validation')

test_datagen = ImageDataGenerator(rescale = 1./255.)
test_generator = test_datagen.flow_from_directory(
    '/content/seg_test/seg_test', 
    target_size=(150,150),
    batch_size=32,
    class_mode='sparse',
    seed=2209
) 

test1_datagen = ImageDataGenerator(rescale = 1./255.)
test1_generator = test1_datagen.flow_from_directory(
    '/content/seg_test/seg_test', 
    target_size=(150,150),
    batch_size=1,
    class_mode=None,
    shuffle=False,
    seed=2209
)

import os
import random as rn
import numpy as np
import tensorflow as tf

SEED = 2209
os.environ['PYTHONHASHSEED']=str(SEED)
np.random.seed(SEED)
rn.seed(SEED)

"""# Membuat Model

- Membuat model Sequential dengan Conv2D Maxpooling Layer
"""

import tensorflow as tf
from keras import regularizers

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)),
    tf.keras.layers.Conv2D(32,(3,3),activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(3,3),
    
    tf.keras.layers.Conv2D(32,(3,3),activation='relu'),
    tf.keras.layers.Conv2D(32,(3,3),activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),
    
    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),
    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),
   
    tf.keras.layers.Conv2D(128,(3,3),activation='relu'),
    tf.keras.layers.Conv2D(128,(3,3),activation='relu'),
    tf.keras.layers.BatchNormalization(),
    
    tf.keras.layers.Conv2D(256,(3,3),activation='relu',padding='same'),
    tf.keras.layers.Conv2D(256,(3,3),activation='relu',padding='same'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.4),
  
    tf.keras.layers.Flatten(),

    tf.keras.layers.Dense(512,activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(128,activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(512,activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
    
    tf.keras.layers.Dense(6,activation='softmax')
])
model.summary()

"""- Membuat fitur callback"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.92):
      print("\nAkurasi telah mencapai >92%!")
      self.model.stop_training = True
callbacks = myCallback()

"""- membuat compile model """

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""- melakukan train model"""

history=model.fit_generator(
  train_generator,
  steps_per_epoch=int(12632/64),
  epochs=50,
  validation_data=validation_generator,
  validation_steps=int(1402/64),
  callbacks=[callbacks],
  verbose=1)

"""# Membuat plot evaluasi akurasi """

import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.figure(figsize=(14, 4))

plt.subplot(1, 2, 1)
plt.title('Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.plot(acc, label='Akurasi Training')
plt.plot(val_acc, label='Akurasi Validasi', linestyle='--')
plt.legend()

plt.subplot(1, 2, 2)
plt.title('Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(loss, label='Akurasi Training')
plt.plot(val_loss, label='Akurasi Validasi', linestyle='--')
plt.legend()

plt.show()

"""# Melakukan uji coba prediksi gambar"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

uploaded = files.upload() 
for fn in uploaded.keys():
  path = fn
  img = image.load_img(path, target_size=(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
 
  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)
  
  print(fn)
  if classes[0][0]==1:
    print('Gambar ini adalah buildings')
  elif classes[0][1]==1:
    print('Gambar Ini adalah forest')
  elif classes[0][2]==1:
    print('Gambar ini adalah glacier')
  elif classes[0][3]==1:
    print('Gambar ini adalah mountain')
  elif classes[0][4]==1:
    print('Gambar ini adalah sea')  
  elif classes[0][5]==1:
    print('Gambar ini adalah street')  
  else:
    print('unknown')

"""# Membuat TFlite"""

# Mengconvert Model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Menyimpann model 
with open('RPS_model.tflite', 'wb') as f:
  f.write(tflite_model)

"""# Identitas diri 
- Nama : Mochamad Febrian Adhi Patria
- Peserta Digitalent Scholarship Kominfo 2021
- Proyek Akhir : Image Classification Model Deployment

"""